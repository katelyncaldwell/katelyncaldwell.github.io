
# Week 1
## Picking a Topic: From Commonsense to NLP

In the months leading up to beginning the DREAM program, I read a number of publications on commonsense in AI from the [MOSAIC team at The Allen Institute](https://mosaic.allenai.org/). They led me to related topics in natural language models, especially research in large transformer models and their applications. In this post, I will describe a little about the topics that interested me originally and how they led me to my research topic. 

### Commonsense in AI

Using machine learning, a computer can accomplish *specific* tasks. Given labeled data, the computer can draw connections between the data and labels to identify patterns to predict labels for unseen data. This knowledge is limited to the scope of the labels and lacks any contextual or implicit knowledge. Humans, on the other hand, begin collecting common sense from a very early age. One example, I understand that the sentence “She bought medicine because” should obviously end with “she was ill,” rather than “she had money.” Similarly, I might use context to understand why a joke is funny. 

<img src="https://github.com/katelyncaldwell/katelyncaldwell.github.io/blob/master/images/commonsenseJoke.png" alt="drawing" width="400"/>

Commonsense in AI has been an open question in AI research for decades with few promising developments. Ambitious researchers once tried to create an accessible database to compute common sense, but these efforts failed — the body of knowledge required is too vast to hard code. Recently, though, as the amount of data and computing power increased — deep neural nets have opened up promising pathways to develop AI common sense. Specifically, large pre-trained language models have been increasingly performant on a number of downstream commonsense tasks in language. These tasks led researchers to investigate ways to optimize models for commonsense. In one paper, for instance, — *[Comet: Commonsense Transformers for Automatic Knowledge Graph Construction](https://aclanthology.org/P19-1470.pdf)* — the author used these language models to construct commonsense knowledge graphs — databases which link entities by commonsense knowledge relations like ‘causes’ or ‘needs’ (check out the [demo](https://mosaickg.apps.allenai.org/model_comet2020_people_events/?head=PersonX%20acts%20quickly&rel=xReason&rel=xWant&rel=xEffect) — it’s really cool). 


<img src="https://github.com/katelyncaldwell/katelyncaldwell.github.io/blob/master/images/commonsenseGraph.png" alt="drawing" width="600"/>

Photo from the original [COMET](https://aclanthology.org/P19-1470/) paper 

This tool makes certain implicit commonsense relations explicit, which makes them accessible to humans and therefore, to computers. This tool has already been used for a number of tasks requiring commonsense knowledge—e.g. [sarcastic comment generation](https://arxiv.org/pdf/2004.13248.pdf) and [automated storytelling](https://arxiv.org/pdf/2009.00829.pdf). This tool relies on recent advancements in NLP — next week, I will discuss the Big Language Models behind these developments.
